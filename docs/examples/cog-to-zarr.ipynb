{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Converting a Cloud-Optimized GeoTIFF to Zarr\n",
    "\n",
    "This notebook walks through a complete end-to-end workflow: opening a remote Cloud-Optimized GeoTIFF, extracting geospatial metadata, writing a multi-resolution Zarr V3 store with all three conventions (proj:, spatial:, multiscales), and validating the result.\n",
    "\n",
    "We use the Sentinel-2 L2A TCI (true-color) band from the [async-geotiff example](https://github.com/developmentseed/async-geotiff#example).\n",
    "\n",
    "**Prerequisites:** [The proj: Convention](proj-convention.ipynb) | [Composition](composition.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-header",
   "metadata": {},
   "source": [
    "## Step 1: Open the remote COG\n",
    "\n",
    "We use [async-geotiff](https://github.com/developmentseed/async-geotiff) to open the Cloud-Optimized GeoTIFF directly from S3. The GeoTIFF object exposes the geospatial properties we need — `crs`, `transform`, `bounds`, and `shape` — without reading any pixel data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch2yar9qfvi",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from geozarr_toolkit import (\n",
    "    MultiscalesConventionMetadata,\n",
    "    ProjConventionMetadata,\n",
    "    SpatialConventionMetadata,\n",
    "    create_multiscales_layout,\n",
    "    create_proj_attrs,\n",
    "    create_spatial_attrs,\n",
    "    create_zarr_conventions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4kg0z9ducs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to write to S3, False to use a local store\n",
    "USE_S3 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bzg24fl312w",
   "metadata": {},
   "outputs": [],
   "source": [
    "from async_geotiff import GeoTIFF\n",
    "from obstore.store import S3Store\n",
    "\n",
    "store = S3Store(\"sentinel-cogs\", region=\"us-west-2\", skip_signature=True)\n",
    "path = \"sentinel-s2-l2a-cogs/12/S/UF/2022/6/S2B_12SUF_20220609_0_L2A/TCI.tif\"\n",
    "\n",
    "geotiff = await GeoTIFF.open(path, store=store)\n",
    "\n",
    "print(f\"CRS:       {geotiff.crs}\")\n",
    "print(f\"Transform: {geotiff.transform}\")\n",
    "print(f\"Shape:     {geotiff.shape}\")\n",
    "print(f\"Bounds:    {geotiff.bounds}\")\n",
    "print(f\"Bands:     {geotiff.count}\")\n",
    "print(f\"Dtype:     {geotiff.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ipiacy1igg",
   "metadata": {},
   "source": [
    "### Step 2: Build convention metadata from the COG\n",
    "\n",
    "The GeoTIFF's properties map directly to convention attributes. The COG also contains internal **overviews** (reduced-resolution copies) which map naturally to the **multiscales** convention — each overview becomes a scale level.\n",
    "\n",
    "- `geotiff.crs.to_epsg()` → `proj:code`\n",
    "- `geotiff.transform` (Affine coefficients) → `spatial:transform`\n",
    "- `geotiff.shape` → `spatial:shape`\n",
    "- `geotiff.bounds` → `spatial:bbox`\n",
    "- `geotiff.overviews` → `multiscales` layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-attrs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build proj: and spatial: attributes from the GeoTIFF's properties\n",
    "t = geotiff.transform\n",
    "\n",
    "geozarr_attrs = create_proj_attrs(code=f\"EPSG:{geotiff.crs.to_epsg()}\")\n",
    "geozarr_attrs.update(\n",
    "    create_spatial_attrs(\n",
    "        dimensions=[\"Y\", \"X\"],\n",
    "        bbox=list(geotiff.bounds),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Build multiscales layout from the COG's overviews\n",
    "# The base (full-resolution) image is level 0; each overview is a coarser level.\n",
    "base_res = t.a  # pixel width of the base level\n",
    "levels = [\n",
    "    {\"asset\": \"0\", \"transform\": {\"scale\": [1.0, 1.0], \"translation\": [0.0, 0.0]}},\n",
    "]\n",
    "for i, overview in enumerate(geotiff.overviews):\n",
    "    ov_res = overview.transform.a\n",
    "    scale_factor = ov_res / base_res\n",
    "    levels.append(\n",
    "        {\n",
    "            \"asset\": str(i + 1),\n",
    "            \"derived_from\": \"0\",\n",
    "            \"transform\": {\n",
    "                \"scale\": [scale_factor, scale_factor],\n",
    "                \"translation\": [0.0, 0.0],\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "\n",
    "geozarr_attrs.update(create_multiscales_layout(levels))\n",
    "geozarr_attrs[\"zarr_conventions\"] = create_zarr_conventions(\n",
    "    MultiscalesConventionMetadata(),\n",
    "    ProjConventionMetadata(),\n",
    "    SpatialConventionMetadata(),\n",
    ")\n",
    "\n",
    "print(f\"Base resolution: {base_res} m\")\n",
    "print(f\"Overview levels: {len(geotiff.overviews)}\")\n",
    "for i, overview in enumerate(geotiff.overviews):\n",
    "    print(\n",
    "        f\"  Overview {i+1}: {overview.width}x{overview.height} px, {overview.transform.a:.1f} m/px\"\n",
    "    )\n",
    "print()\n",
    "print(json.dumps(geozarr_attrs, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x64krb25pvn",
   "metadata": {},
   "source": [
    "### Step 3: Read and write to Zarr V3 with multiscales\n",
    "\n",
    "We read the full-resolution image and each overview, writing them as separate child arrays in a Zarr V3 store. Set `USE_S3` above to control the output destination:\n",
    "\n",
    "- **`USE_S3 = True`**: writes to a remote S3 bucket via obstore's `S3Store`\n",
    "- **`USE_S3 = False`**: writes to a local directory via Zarr's `LocalStore`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rc2uzn39va",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "from zarr.storage import LocalStore, ObjectStore\n",
    "\n",
    "bucket = \"us-west-2.opendata.source.coop\"\n",
    "prefix = \"pangeo/geozarr-examples/TCI.zarr\"\n",
    "local_path = \"data/TCI.zarr\"\n",
    "\n",
    "if USE_S3:\n",
    "    output_store = S3Store(bucket, prefix=prefix, region=\"us-west-2\")\n",
    "    zarr_store = ObjectStore(output_store)\n",
    "else:\n",
    "    zarr_store = LocalStore(local_path)\n",
    "\n",
    "root: zarr.Group = zarr.open_group(zarr_store, mode=\"w\", zarr_format=3)\n",
    "\n",
    "# Set convention attributes on the group\n",
    "root.attrs.update(geozarr_attrs)\n",
    "\n",
    "# Write the full-resolution image as level \"0\"\n",
    "base_array = await geotiff.read()\n",
    "root.create_array(\"0\", data=base_array.data, chunks=(3, 512, 512))\n",
    "print(f\"Level 0 (base): shape={base_array.data.shape}, dtype={base_array.data.dtype}\")\n",
    "\n",
    "# Write each overview as a separate level\n",
    "for i, overview in enumerate(geotiff.overviews):\n",
    "    ov_array = await overview.read()\n",
    "    root.create_array(str(i + 1), data=ov_array.data, chunks=(3, 512, 512))\n",
    "    print(f\"Level {i+1} (overview): shape={ov_array.data.shape}\")\n",
    "\n",
    "location = f\"s3://{bucket}/{prefix}\" if USE_S3 else local_path\n",
    "print(f\"\\nWrote Zarr V3 store to {location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lve85227co",
   "metadata": {},
   "source": [
    "### Step 4: Validate the Zarr store\n",
    "\n",
    "We reopen the store and use `validate_group` to confirm the conventions are correctly applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rj1mowgsd3j",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geozarr_toolkit import detect_conventions, validate_group\n",
    "\n",
    "# Reopen and validate\n",
    "if USE_S3:\n",
    "    read_store = S3Store(bucket, prefix=prefix, region=\"us-west-2\", skip_signature=True)\n",
    "    zarr_store = ObjectStore(read_store)\n",
    "else:\n",
    "    zarr_store = LocalStore(local_path)\n",
    "\n",
    "root = zarr.open_group(zarr_store, mode=\"r\")\n",
    "\n",
    "detected = detect_conventions(dict(root.attrs))\n",
    "print(f\"Detected conventions: {detected}\")\n",
    "\n",
    "results = validate_group(root)\n",
    "for conv, errors in results.items():\n",
    "    status = \"PASS\" if not errors else \"FAIL\"\n",
    "    print(f\"  [{status}] {conv}\")\n",
    "    for err in errors:\n",
    "        print(f\"         {err}\")\n",
    "\n",
    "print(f\"\\nStore tree:\")\n",
    "root.tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the full workflow from COG to convention-compliant Zarr V3:\n",
    "\n",
    "1. **Open** a remote COG with async-geotiff (no pixel data read)\n",
    "2. **Extract** CRS, transform, bounds, and overview structure\n",
    "3. **Map** these properties to proj:, spatial:, and multiscales convention attributes\n",
    "4. **Write** the full image and all overview levels to a remote Zarr V3 store on S3\n",
    "5. **Validate** that the store conforms to all three conventions\n",
    "\n",
    "The same pattern applies to any georeferenced raster — the convention attributes are derived from standard geospatial properties that every GeoTIFF provides."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geozarr-examples (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
